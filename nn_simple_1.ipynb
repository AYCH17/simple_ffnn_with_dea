{"cells":[{"cell_type":"code","execution_count":319,"metadata":{},"outputs":[],"source":["import os\r\n","import sys\r\n","import torch\r\n","import copy\r\n","from torch import nn\r\n","from torch.utils.data import DataLoader\r\n","from torchvision import datasets\r\n","from torchvision.transforms import ToTensor, Lambda, Compose, Normalize\r\n","import matplotlib.pyplot as plt\r\n","import numpy as np\r\n","from tqdm import tqdm "]},{"cell_type":"code","execution_count":320,"metadata":{},"outputs":[],"source":["# on a seulement besoin de l'exécuter une fois\n","# on va faire des appels natifs\n","if not os.path.exists(\"MNIST\"):\n","    if \"linux\" in sys.platform:\n","        os.system(\"wget www.di.ens.fr/~lelarge/MNIST.tar.gz\")\n","        os.system(\"tar -zxvf MNIST.tar.gz\")\n","        os.system(\"rm MNIST.tar.gz\")\n","    elif \"win32\" in sys.platform: \n","        os.system('pwsh -command \"Invoke-WebRequest http://www.di.ens.fr/~lelarge/MNIST.tar.gz -OutFile MNIST.tar.gz\"')\n","        os.system('pwsh -command \"tar -zxvf MNIST.tar.gz\"')\n","        os.system('pwsh -command \"rm MNIST.tar.gz\"')\n","    else:\n","        print(\"tough luck buddy!\")\n"]},{"cell_type":"code","execution_count":321,"metadata":{},"outputs":[],"source":["# Chargement du dataset MNIST\n","# on va appliquer les opérations de conversion ici pour fins de rapidité\n","train_data = datasets.MNIST(\n","    root=\"./\", # le dossier racine où se trouve le dossier MNIST \n","    train=True, # les images d'entrainement\n","    download=False, # pas besoin de télécharger\n","    transform=Compose([\n","        ToTensor(), # Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] \n","        Lambda(lambda x : torch.flatten(x))]) # on \"écrase\" l'image pour retourner un vecteur contenant les pixels\n","    )\n","\n","test_data = datasets.MNIST(\n","    root=\"./\", \n","    train=False, # les images de validation\n","    download=False,\n","    transform=Compose([\n","        ToTensor(), # https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor\n","        Lambda(lambda x : torch.flatten(x))]) # https://pytorch.org/docs/stable/generated/torch.flatten.html\n","    )\n"]},{"cell_type":"code","execution_count":322,"metadata":{},"outputs":[],"source":["# Création des dataloader\n","# On va utiliser les dataloader pour charger les images dynamiquement et appliquer les transformations désirées.\n","# Dans notre cas, la transformation est torchvision.transforms.ToTensor()\n","# C'est la façon privilégiée de faire, en particulier lorsqu'on a de grosses bases de données qui ne peuvent pas être complètement stockées en mémoire vive.\n","# On va tricher sur le batch size pour simplifier l'entrainement : on va tout charger en mémoire (ce sont des petites images donc ça va aller)\n","train_batch_size = 60000\n","test_batch_size = 10000\n","train_dataloader = DataLoader(train_data, batch_size=train_batch_size, pin_memory=True)\n","test_dataloader = DataLoader(test_data, batch_size=test_batch_size, pin_memory=True)\n","\n","train_set = train_dataloader.dataset.data.flatten(-2)/255.\n","train_targets = train_dataloader.dataset.targets\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n"]},{"cell_type":"code","execution_count":323,"metadata":{},"outputs":[],"source":["class NeuralNet(nn.Module):\r\n","    \"\"\"Implémente un réseau de neurones linéaire très simple (perceptron multicouche),\r\n","       inspiré de celui de 3blue1brown.\r\n","\r\n","       trois couches pleinement connectées\r\n","       \r\n","       784 -> 16 -> 16 -> 10\r\n","\r\n","       activation sigmoide et logits obtenus à l'aide de LogSoftMax\r\n","\r\n","       les gradients sont désactivés pour permettre à l'étudiant d'entrainer\r\n","       le réseau à l'aide de métaheuristiques.\r\n","\r\n","    Args:\r\n","        nn (torch.nn.Module): hérite de cette classe, (pas obligatoire, mais ça facilite les choses)\r\n","    \"\"\"\r\n","    def __init__(self):\r\n","        \"\"\"Initialise le réseau de neurones\r\n","        \"\"\"\r\n","        super(NeuralNet, self).__init__()\r\n","        \r\n","        # les différentes couches\r\n","        self.fc1 = nn.Linear(28*28, 16)\r\n","        self.fc2 = nn.Linear(16, 16)\r\n","        self.fc3 = nn.Linear(16, 10)\r\n","\r\n","        # la fonction pour les logits (retourne le \"score\" des classes)\r\n","        self.logsoftmax = nn.LogSoftmax(dim=0)\r\n","        # la fonction pour calculer le loss (l'erreur de prédiction)\r\n","        self.loss_fn = nn.CrossEntropyLoss()\r\n","\r\n","    @torch.no_grad()\r\n","    def forward(self, x:torch.Tensor) -> torch.Tensor:\r\n","        \"\"\"\r\n","        Calcule le résultat du réseau de neurones sur une ou plusieurs images.\r\n","\r\n","        Args:\r\n","            x (torch.Tensor): l'image d'entrée, normalisée et écrasée, taille [B,N] où:\r\n","            - B est le batch size   \r\n","            - N le nombre de pixel\r\n","\r\n","        Returns:\r\n","            torch.Tensor: le résultat de taille [B,C] du traitement par le réseau où:\r\n","            - B est le batch size\r\n","            - C est le nombre de classes \r\n","        \"\"\"\r\n","        # on \"applatit\" l'image, le -2 indique que l'on joins les deux dernières dimensions, \r\n","        # la largeur et la longueur pour que les pixels soient dans la même dimension \r\n","        x = torch.sigmoid(self.fc1(x)) # activation sur la première couche\r\n","        x = torch.sigmoid(self.fc2(x)) # activation sur la deuxième couche\r\n","        x = self.fc3(x) # calcul de la dernière couche\r\n","        return x\r\n","\r\n","    def loss(self, x:torch.Tensor, y:torch.Tensor) -> torch.Tensor:\r\n","        \"\"\"Fonction pour retourner le loss de notre réseau sur un ensemble de prédictions.\r\n","\r\n","        Args:\r\n","            x (torch.Tensor): le tenseur avec les prédictions\r\n","            y (torch.Tensor): la solution\r\n","\r\n","        Returns:\r\n","            torch.Tensor: la perte calculée selon le cross entropy loss: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\r\n","        \"\"\"\r\n","        return self.loss_fn(x, y)\r\n","\r\n","    def get_weights_and_bias(self) -> torch.Tensor:\r\n","        \"\"\"Fonction utilitaire qui retourne un vecteur concaténant les poids et biais de notre réseau de neurones.\r\n","\r\n","        Returns:\r\n","            torch.Tensor: un vecteur de dimension 16*784+16*16+10*16+16+16+10 qui représente une concaténation aplatie\r\n","             des poids des couches fc1,fc2,fc3 et des biais des couches fc1,fc2,fc3\r\n","        \"\"\"\r\n","        return  torch.cat((\r\n","            self.fc1.weight.data.flatten(),\r\n","            self.fc2.weight.data.flatten(), \r\n","            self.fc3.weight.data.flatten(), \r\n","            self.fc1.bias.data.flatten(), \r\n","            self.fc2.bias.data.flatten(), \r\n","            self.fc3.bias.data.flatten()\r\n","            ))\r\n","\r\n","    @torch.no_grad()\r\n","    def set_weights_and_bias(self, x) -> None:\r\n","        \"\"\"Fonction utilitaire pour mettre à jour les poids et les biais du réseau de neurones\r\n","           à partir d'un individu issu d'un algorithme d'optimisation quelconque.\r\n","\r\n","           On va donc extraire et remettre en forme les sections respectives du vecteur et les assigner aux couches correspondantes\r\n","\r\n","        Args:\r\n","            x (torch.Tensor): un vecteur de dimension 16*784+16*16+10*16+16+16+10 qui représente une concaténation aplatie\r\n","             des poids des couches fc1,fc2,fc3 et des biais des couches fc1,fc2,fc3\r\n","        \"\"\"\r\n","        # les index des fin poids et des biais\r\n","        iw1,iw2,iw3,ib1,ib2,ib3 = 12544, 12800, 12960, 12976, 12992, 13002\r\n","        \r\n","        self.fc1.weight.data = torch.from_numpy(np.reshape(x[0:iw1],(16, 784)))\r\n","        self.fc2.weight.data = torch.from_numpy(np.reshape(x[iw1:iw2],(16, 16)))\r\n","        self.fc3.weight.data = torch.from_numpy(np.reshape(x[iw2:iw3],(10, 16)))\r\n","       \r\n","        self.fc1.bias.data = torch.from_numpy(x[iw3:ib1])\r\n","        self.fc2.bias.data = torch.from_numpy(x[ib1:ib2])\r\n","        self.fc3.bias.data = torch.from_numpy(x[ib2:ib3])\r\n","\r\n","    @torch.no_grad()\r\n","    def fonction_objective(self, dataloader:DataLoader) -> torch.Tensor:\r\n","        \"\"\"Bonne pratique pour le chargement de données -- Ne pas utiliser pour cet exercice\r\n","\r\n","        Args:\r\n","            dataloader (DataLoader): le dataloader qui contient les données pour évaluation/entrainement\r\n","\r\n","        Returns:\r\n","            torch.Tensor: le loss/le score du réseau de neurones\r\n","        \"\"\"\r\n","        loss = torch.zeros(1)\r\n","\r\n","        for batch, (X,y) in enumerate(dataloader):\r\n","            # on calcule l'erreur de prédiction\r\n","            pred = self(X)\r\n","            loss += self.loss(pred, y) # le loss est le résultat de la fonction objective, on cherche à le minimiser\r\n","\r\n","        # on va calculer le loss moyen\r\n","        #loss = loss / len(dataloader)\r\n","        return loss\r\n","    \r\n","    @torch.no_grad()\r\n","    def fast_fonction_objective(self, x:torch.tensor, targets:torch.tensor) -> torch.Tensor:\r\n","        \"\"\"utiliser cette fonction pour le calcul du score du réseau de neurones\r\n","\r\n","        Args:\r\n","            x (torch.tensor): un tenseur de taille BxN contenant les données pour évaluation/entrainement\r\n","            targets (torch.tensor): un vecteur de taille B contenant la classe de chaque image\r\n","\r\n","        Returns:\r\n","            torch.Tensor: le loss/le score du réseau de neurones\r\n","        \"\"\"\r\n","        loss = torch.zeros(1)\r\n","\r\n","        # on calcule l'erreur de prédiction\r\n","        pred = self(x.double())\r\n","        return self.loss(pred, targets) # le loss est le résultat de la fonction objective, on cherche à le minimiser\r\n","\r\n","\r\n","    @torch.no_grad()\r\n","    def run_objective_on_train(self,x):\r\n","        self.set_weights_and_bias(x)\r\n","        return self.fast_fonction_objective(train_set,train_targets)\r\n","   \r\n"]},{"cell_type":"code","execution_count":324,"metadata":{},"outputs":[],"source":["# Fonction d'évaluation de la performance de notre réseau de neurones\n","# https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n","def test(dataloader, model): \n","    size = len(dataloader.dataset)\n","    model.eval()\n","    test_loss, correct = 0, 0\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            X, y = X.to(device), y.to(device)\n","            pred = model(X.double())\n","            test_loss += model.loss_fn(pred, y).item()\n","            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","    print(\"New\\n\",correct,size)\n","    test_loss /= size\n","    correct /= size\n","    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")        \n"]},{"cell_type":"code","execution_count":325,"metadata":{},"outputs":[],"source":["# Comme le dataloader est lent et qu'on peut stocker MNIST en mémoire, on va le faire pour \n","# rendre le calcul de la fonction objective beaucoup plus rapide.\n","# Il est donc recommandé d'utiliser la fonction fast_fonction_objective pour obtenir le loss \n","\n","\n","# pour calculer le loss/fitness du réseau sur les images\n","# appelez \n","# remarquez que si vous voulez évaluer plusieurs individus en même temps que vous pourriez instancier plusieurs NeuralNet...\n","# ce n'est pas vraiment possible pour les réseaux énormes, mais dans ce cas ça devrait aller...\n"]},{"cell_type":"code","execution_count":326,"metadata":{},"outputs":[],"source":["##### IMPLÉMENTEZ VOTRE ALGORITHME ET ROUTINE D'ENTRAINEMENT ICI\n"]},{"cell_type":"code","execution_count":327,"metadata":{},"outputs":[],"source":["def train_with_dea(fobj, popsize, dim, its, F=0.8, crossp=0.7):\r\n","    NN = NeuralNet()\r\n","    pop = np.random.rand(popsize, dim)\r\n","    fitness = np.asarray([])\r\n","    \r\n","    for k in range(len(pop)) : \r\n","        fitness = np.append(fitness,fobj(NN, pop[k]))\r\n","    \r\n","    for i in tqdm(range(its)):\r\n","        for j in range(popsize):\r\n","             idxs = [idx for idx in range(popsize) if idx != j]\r\n","             r1 , r2, r3 = np.random.choice(idxs, 3, replace = False)\r\n","             l_rand = np.random.randint(0, dim)\r\n","             new_elem = np.empty((0,dim))\r\n","             for l in range(dim):\r\n","                 new_elem = np.append(new_elem,pop[r3,l] + F * (pop[r1,l] - pop[r2,l]) if np.random.rand() < crossp or l == l_rand else pop[j,l])\r\n","             \r\n","             fitness_new_elem = fobj(NN, new_elem)\r\n","             if fitness_new_elem < fitness[j]:\r\n","                 pop[j] = new_elem\r\n","                 fitness[j] = fitness_new_elem\r\n","    \r\n","    best_elem_idx = np.argmin(fitness) \r\n","    best_elem = pop[best_elem_idx]\r\n","    best_fitness = fitness[best_elem_idx]\r\n","\r\n","    return best_elem , best_fitness\r\n","\r\n","            \r\n","      "]},{"cell_type":"code","execution_count":329,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 10/10 [00:43<00:00,  4.32s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[ 0.81200916 -0.08339073  0.72890795 ...  1.13350873  0.62401702\n","  0.70185476] 2.507638259128103\n","New\n"," 974.0 10000\n","Test Error: \n"," Accuracy: 9.7%, Avg loss: 0.000251 \n","\n"]}],"source":["\r\n","popsize = input(\"Entrer la taille de la population\")\r\n","its = input(\"Entrer le nombre de generations : \")\r\n","\r\n","##Train\r\n","best_weights_bias , score = train_with_dea(fobj=NeuralNet.run_objective_on_train,dim=13002,popsize=int(popsize),its= int(its))\r\n","print(best_weights_bias,score)\r\n","\r\n","#Test\r\n","NN =NeuralNet()\r\n","NN.set_weights_and_bias(best_weights_bias)\r\n","test(test_dataloader,NN)\r\n"]},{"cell_type":"markdown","metadata":{},"source":["## References\r\n","https://www.researchgate.net/figure/Pseudocode-of-the-differential-evolution-algorithm-DE-rand-1-bin_fig2_28601722\r\n"]}],"metadata":{"interpreter":{"hash":"2a6ed1285f5ba108d72d5ff2c469f70bfbcd02f5c6d189e0a2739924c28b3ac1"},"kernelspec":{"display_name":"Python 3.9.4 64-bit","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.4"},"orig_nbformat":2},"nbformat":4,"nbformat_minor":2}